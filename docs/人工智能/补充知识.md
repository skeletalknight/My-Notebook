

## 补充知识

### 归一化

- **定义**：对输入/中间特征进行**线性变换**，使其符合特定分布（通常零均值、单位方差）。
- **核心作用**：
	- ✅ **加速训练**：允许更大学习率，损失函数更平滑。
	- ✅ **稳定训练**：缓解梯度消失/爆炸，平衡各层梯度幅度。
	- ⚠️ **轻微正则化**（如BatchNorm）：依赖Batch统计量引入噪声。
- **标准公式**：$\hat{x} = \frac{x - \mu}{\sigma}, \quad y = \gamma \hat{x} + \beta$
- **梯度缩放**：反向传播时梯度被缩放宽$\frac{1}{\sigma}$，但通过以下机制抵消：
	- 可学习参数γ：动态调整梯度幅度（若某层输入方差大，γ会学习增大值）。
	- ****优化器自适应****：如Adam自动调整参数更新步长。

- **常见归一化方法对比**

| **方法**      | **归一化维度**  | **适用场景**              | **训练/测试差异**          |
| ------------- | --------------- | ------------------------- | -------------------------- |
| **BatchNorm** | Batch维度 (N)   | CNN、固定结构数据，大批量 | 使各层梯度量级相近         |
| **LayerNorm** | 特征维度 (C)    | RNN/Transformer、小批量   | 平衡同一位置不同特征的梯度 |
| **GroupNorm** | 分组特征 (C//G) | 小批量图像数据            | 无                         |

