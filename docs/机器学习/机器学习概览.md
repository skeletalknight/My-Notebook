# 统计机器学习

## 概述

**统计机器学习**是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。



## 统计学习分类

### 按数据类型分类

本节课内容主要以监督学习和无监督学习为主

- 监督学习（Supervised learning）
	- 从**标注数据**中学习预测模型，本质是学习输入与输出的映射的统计规律
	- 假设输入与输出的随机变量$X$和$Y$遵循联合概率分布$P(X,Y)$，并且数据是简单抽样结果
	- 决策函数：$Y=f(X)$，有回归和分类两种类型
- 无监督学习（Unsupervised learning）
	- 指从**无标注数据**中学习预测模型，本质是学习数据的统计规律或者潜在结构
	- 模型函数：$z=g(x)$，典型模型如聚类算法
- 半监督学习（Semi-supervised learning）
	- 少量标注数据，大量未标注数据
	- 利用未标注数据的信息，辅助标注数据，进行**监督学习**
- 强化学习（Reinforcement learning）
	- 指智能系统在与环境的**连续互动**中学习**最优行为策略**的机器学习问题。
	- 具有状态转移概率函数、奖励函数、策略函数等

### 按模型分类

- 概率模型与非概率模型：
	- 区别不在与输入与输出之间的映射关系，而在于模型的内在结构，概率模型一定可以表示为**联合概率分布**的形式
	- 模型结构：监督学习中，概率模型取条件概率分布形式$ P(y|x)$， 非概率模型取函数形式$y=f(x)$
	- 概率模型举例：决策树，朴素贝叶斯，隐马尔可夫模型，概率潜在语义分析，高斯混合模型
	- 非概率模型举例：感知机，支持向量机，k近邻，AdaBoost，k均值，潜在语义分析，神经网络
- 线性模型与非线性模型
	- 区别在模型的函数是否是线性
- 参数模型与非参数模型
	- 区别在于模型参数是否固定，非参数模型维度不固定或者无穷大，随着训练数据量的增加而不断增大。



## 统计学习策略

### 损失函数

损失函数用于衡量一次预测的好坏 $L(Y, f(X))$ 或者 $L(Y, P(Y|X))$，损失函数值越小，模型越好

常用损失函数：

- 0-1损失函数 $L(Y,f(X)) = \begin{cases} 1, & Y \ne f(X) \\ 0, & Y = f(X) \end{cases}$

- 平方损失函数 $L(Y,f(X)) = (Y - f(X))^2$

- 绝对损失函数 $L(Y,f(X)) = |Y - f(X)|$

- 对数损失函数（交叉熵损失） $L(Y,P(Y|X)) = -\log P(Y=k|X)$

### 风险策略

由于存在训练集和测试集（真实应用）的差异，模型有欠拟合和过拟合的风险

为了避免或减轻过拟合现象，须要使用额外的技巧（如模型选择、交叉验证、提前停止、正则化、剪枝、dropout）

基于是否具有正则项，我们也有以下两种风险策略

**经验风险最小化**(ERM empirical risk minimization)，指在样本空间内最小化损失函数。

经验风险最小化能够保证有较好的学习效果。但当样本容量很小时，经验风险最小化学习效果未必好，可能产生过拟合。

最大似然估计是一个很好的经验风险最小化实例，其所使用的模型是一个条件概率分布函数。

$$
\min_{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i))
$$
**结构风险最小化** (SRM structure risk minimization)，是为防止过拟合提出的策略。

等价于正则化 (regularization)，具有正则化项，描述模型的复杂度，其是定义在假设空间上的泛函。而正则化参数 (惩罚系数) $\lambda \ge 0$，用于权衡经验风险和模型复杂度。
$$
R_{\text{srm}}(f) = \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i)) + \lambda J(f)
$$