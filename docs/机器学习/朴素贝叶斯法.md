# 朴素贝叶斯法

## 概述

朴素贝叶斯法（Naive Bayes），这是一种基于贝叶斯定理与特征条件独立性假设的分类方法。

朴素贝叶斯法基于特征条件独立假设，在训练集上学习输入输出的联合概率，并以后验概率作为模型。

朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。

## 学习与分类

### 基本思想

对于输入向量$X$和标签$Y$，我们需要求其后验分布$P(Y=c_k|X=x)$，有贝叶斯定理：
$$
\begin{aligned}
P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_{k} P(X=x|Y=c_k)P(Y=c_k)} && (4.4)
\end{aligned}
$$
但是，由于$P(X=x|Y=c_k)$参数量过大，因此我们引入**条件独立假设**，也就是说，$X$的特征对于$Y$的条件分布是独立的

进而，后验分布可以改进为
$$
\begin{aligned}
P(Y=c_k|X=x) = \frac{P(Y=c_k) \prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_{k} P(Y=c_k) \prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)}, && k=1,2,\dots,K
\end{aligned}
$$
我们选取后验分布概率最大的为预测值。

### 后验分布概率最大化的意义

实际上后验概率最大化等价于经验风险最小化。采用0-1损失函数有

$$
\begin{aligned}
L(Y,f(X)) = \begin{cases} 1, & Y \ne f(X) \\ 0, & Y = f(X) \end{cases}
\end{aligned}
$$

此时为了使经验风险最小化，需要取决策函数如下（即最大化后验概率分布）
$$
\begin{aligned} f(x) &= \arg \min_{y \in \mathcal{Y}} \sum_{k=1}^{K} L(c_k, y) P(c_k|X=x) \\ &= \arg \min_{y \in \mathcal{Y}} \sum_{k=1}^{K} P(y \ne c_k|X=x) \\ &= \arg \max_{y \in \mathcal{Y}} P(y=c_k|X=x) \end{aligned}
$$
## 参数估计

### 极大似然估计

在朴素贝叶斯法中，学习意味着估计$P(X=x|Y=c_k),\; P(Y=c_k)$

我们可以简单地，直接采用极大似然估计，有

$$
\begin{aligned}
P(Y=c_k) &= \frac{\sum_{i=1}^{N} I(y_i=c_k)}{N}，\\
P(X^{(j)}=a_{jl}|Y=c_k) &= \frac{\sum_{i=1}^{N} I(x_i^{(j)}=a_{jl}, y_i=c_k)}{\sum_{i=1}^{N} I(y_i=c_k)} 
\end{aligned}
$$

特别的，在实际计算中，我们只需要求出不同标签对应的分子进行比较，可以不关注归一化因子

即，在求出以上内容后，对于每个$i$对应的$Y=c_i$，比较
$$
P(Y=c_i) \prod_{j} P(X^{(j)}=x^{(j)}|Y=c_i)
$$

### 贝叶斯估计

用极大似然估计可能会出现所要估计的**概率值为0**的情况。这时使分类产生偏差。

解决方法是采用贝叶斯估计，即计数时，为所有可能的事件计数都加上一个小的正数 $\lambda$

具体地，条件概率的贝叶斯估计是：
$$
\begin{aligned}
P_{\lambda}(Y=c_k) &= \frac{\sum_{i=1}^{N} I(y_i=c_k) + \lambda}{N+K\lambda}\\
P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k) &= \frac{\sum_{i=1}^{N} I(x_i^{(j)}=a_{jl}, y_i=c_k) + \lambda}{\sum_{i=1}^{N} I(y_i=c_k) + S_j \lambda} 
\end{aligned}
$$
需要注意，其中$K$代表标签$Y$的种类；对应的，$S_j$表示$X^{(j)}$的种类（从归一性出发，不要求$Y=c_k$内包含所有$S_j$个种类）

证明该式子，需要假设先验分布为参数为$\lambda$的Dirichlet分布，再对后验分布求期望即可。

当 $\lambda=0$ 时就是极大似然估计。常取 $\lambda=1$，这时称为拉普拉斯平滑 (Laplacian smoothing)。

显然，对任何 $l=1,2,\dots,S_j, k=1,2,\dots,K$，有
$$
\begin{aligned}
&P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k) > 0 \\
&\sum_{l=1}^{S_j} P(X^{(j)}=a_{jl}|Y=c_k) = 1
\end{aligned}
$$
