# LLM概览

## LLM的架构

- Auto Encoder

  从高维编码到低维，在映射回原来的输入，即训练$f(x)=x$，则可以获得一套编码，使用更少的特征的存储语义信息

- Attention

  通过一个矩阵，计算并记录每一个单词和其他单词的影响关系，并将影响加入后续计算。

  为了实现上下文的Attention，空间占用为$O(n^2)$

- Transformer **（2025年了，还是用Transformer吗？）**

  - Encoder-Only (BERT)

    - 作用：理解、分类、标注、特征提取
    - 注意力：双向
    - 机理：通过Encoder生成一个全局的、定长的理解
  - Encoder & Decoder (T5)

    - 作用：翻译、摘要、问答
    - 注意力：双向
    - 机理：基于Encoder的定长理解，再用Decoder逐token生成回答
    - Decoder-Only（GPT）
      - 作用：文本补全，自然语言生成
      - 注意力：单向（前向）
      - 机理：不进行压缩，直接使用原文token进行回答生成
      - 优势：训练迭代速度相对于编码-解码模型快很多
    - Embedding（向量数据库）
      - 作用：文本向量化

- MoE

  Router+Expert，即分类器加对应专家

  Open-AI很可能采用该架构，有利于提高模型总参数、降低计算消耗

- VLM

  多模态模型——将媒体输入变为token，再进行类似的训练

参数量决定了语意向量维度，也就具有更好的表达能力。(但是参数太大，模型表现可能也会下降)

大模型的参数量和训练文本量大致的比例只有1:20，**（到100-1000倍仍有提高？）**

这个比例称为压缩率，受限于架构，超过比例的数据集输入，也难以提高进一步降低loss

GPT4具有1TB的参数量，只需要20TB左右的高质量文本**（那为什么说数据不够？——就是不够）**

思维链的讨论：

正常来说，模型擅长直接给答案——因为训练的文本中，解释内容更少。

O1的训练，是因为出现了将数据集增广为具有思维过程的数据的方法

会将角色设定放入文段开头，并整合序列化为文本对象

有意思的是，我们可以在文本内嵌入

模型构建：窄-宽-窄，因为需要根据输入进行发散思考；同时因为宽的隐藏层，实现恒等映射比较简单，因此还需要进行激活单元数量的限制。

比如，在claude大模型的一个层中的一个参数，仅与金门大桥具有激活关系。



## LLM的训练

- 预训练：
  - 内容：TB级别，通过数据集进行文本填空（一切知识来源）
  - 数据集：低质量，高数量
  - 模型：Base Model
  - 训练需求：需要1k GPU数以月计的训练
- 监督学习：
  - 内容：10-100KB，通过人类标注、先前模型生成，获得`{prompt, response}`对，让模型学会回答格式
  - 数据集：中等质量，低数量
  - 模型：SFT Model
  - 训练需求：需要10 GPU几天的训练量
- :star:奖励学习：**（思考过程/数据如何实现）**
  - 原因：对于o1这样的长思维链回答，需要对不同步骤进行评分
  - 内容：MB级别，人类撰写/算法生成的思维过程
  - 数据集：高质量，中等数量
  - 训练需求：需要10 GPU几天的训练量
- 强化学习：
  - 内容：MB级别，让人类对模型产出的多个内容进行等级分类
  - 数据集：高质量（问题），低数量
  - 训练需求：需要10 GPU几天的训练量
  - 

上交有对提升大模型进行许多方法的探索，结果发现不如征流已有尖端模型输出。

已有模型带模型，比如千问采用DS数据，是非常高效的（这就是o1为什么不放出思维过程）



## LLM的下游应用

具有垂直领域的数据集，希望针对该领域训练对应LLM

- 重新训练
- 微调（常基于Base model）
  - 全参数微调
  - LoRA

- Prompt Engineering
- RAG（常基于向量数据库）

由于基础模型具有最多的知识，而微调/训练会牺牲总能力（以提高领域表现），同时算力消耗更大

常见的更好方式是PE或者RAG